{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC0134 - Applied Machine Learning Systems Assignment 2019/2020\n",
    "## Task B1: Face Shape Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to detect 5 types of face shapes from the *cartoon_set* dataset:\n",
    "1. Build an VGG-16 CNN architecture\n",
    "2. Fit the CNN model to the training data\n",
    "3. Test the performances of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "\n",
    "# Import network libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Build the neural network using VGG-16 architecture\n",
    "1. Build the sequential model and add convolutional and max pooling layers to it.\n",
    "2. Add dropout layer at fully connected layer\n",
    "3. Add softmax layer at the end\n",
    "4. Compile the model and use ADAM (Adaptive learning rate optimization algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 225, 225, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 223, 223, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 223, 223, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 109, 109, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 107, 107, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 107, 107, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 51, 51, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 49, 49, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 47, 47, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 47, 47, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 23, 23, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 21, 21, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 19, 19, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 17, 17, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 33,621,573\n",
      "Trainable params: 33,619,653\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add 1st convolution block\n",
    "model.add(Conv2D(filters=64, input_shape=(227,227,3), kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Add 2nd convolution block\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Add 3rd convolution block\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Add 4th convolution block\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Add 5th convolution block\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Output Layer (5 clases for face shapes)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=5, activation=\"softmax\"))\n",
    "\n",
    "# Display summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Compile the model using ADAM (Adaptive learning rate optimization)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels.csv and convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file_name  face_shape\n",
      "0     0.png           4\n",
      "1     1.png           4\n",
      "2     2.png           3\n",
      "3     3.png           0\n",
      "4     4.png           2\n",
      "  file_name  face_shape\n",
      "0     0.png           5\n",
      "1     1.png           5\n",
      "2     2.png           4\n",
      "3     3.png           1\n",
      "4     4.png           3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Modified the directory\n",
    "os.chdir('..')\n",
    "AMLS_dir = os.path.abspath(os.curdir)\n",
    "basedir = os.path.join(AMLS_dir,'Datasets')\n",
    "cartoon_dir = os.path.join(basedir,'cartoon_set')\n",
    "images_dir = os.path.join(cartoon_dir,'img')\n",
    "labels_path = os.path.join(cartoon_dir,'labels.csv')\n",
    "\n",
    "# Converting csv into dataframe using read_csv(label_path)\n",
    "cartoon_df = pd.read_csv(os.path.normcase(labels_path), sep='\\t', engine='python')\n",
    "df = cartoon_df[['file_name', 'face_shape']]\n",
    "print(df.head())\n",
    "\n",
    "# Convert the face shape column to class 1-5\n",
    "df.loc[:,'face_shape'] += 1\n",
    "print(df.head())\n",
    "\n",
    "# Convert face shape column type from int64 to str for data generator\n",
    "df = df.applymap(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data generators to draws from train, test and validation dataframe\n",
    "1. Convolution Neural Networks fed by batches of images to train, therefore need to convert from (size1, size2, channels) to (samples, size1, size2, channels).\n",
    "2. Split the dataframe into test, train and validation set.\n",
    "3. Use Keras Image Processing API (flow_from_dataframe) to load the images dataset according to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5600 validated image filenames belonging to 5 classes.\n",
      "Found 1400 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=0.7, random_state=42)\n",
    "\n",
    "# Import data generator API \n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Setup data generator for test and train dataset generator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   validation_split=0.2)\n",
    "                                   \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generating training and validation dataset for AlexNet CNN\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "                                                    directory=images_dir,\n",
    "                                                    x_col=\"file_name\",\n",
    "                                                    y_col=\"face_shape\",\n",
    "                                                    target_size=(227, 227),\n",
    "                                                    batch_size=32,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    subset='training')\n",
    "\n",
    "valid_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "                                                    directory=images_dir,\n",
    "                                                    x_col=\"file_name\",\n",
    "                                                    y_col=\"face_shape\",\n",
    "                                                    target_size=(227, 227),\n",
    "                                                    batch_size=32,\n",
    "                                                    shuffle=True, \n",
    "                                                    class_mode='categorical',\n",
    "                                                    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using EarlyStopping and ModelCheckpoint Keras API callback to save the weights\n",
    "Too many training epochs will lead to overfitting the training dataset whereas too few will result underfitting. Therefore, it is better to monitor the performance of the model during training and call EarlyStopping when overfitting occurs and save the weights using ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use early stopping to terminate training epochs through callbacks\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Seek a mininum for validation loss and display the stopped epochs using verbose and adding delays\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "# Save best model using checkpoint\n",
    "B1_dir = os.path.join(AMLS_dir, 'B1')\n",
    "model_path = os.path.join(B1_dir, 'AlexNet.h5')\n",
    "mcp = ModelCheckpoint(os.path.normcase(model_path), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# Define callback function in a list\n",
    "callback_list = [es, mcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fit the VGG-16 CNN to training and validation data\n",
    "Access model training history using history callback to record training metrics for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set steps per epoch for callback \n",
    "STEP_SIZE_TRAIN = train_generator.samples//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.samples//valid_generator.batch_size\n",
    "print(STEP_SIZE_TRAIN)\n",
    "print(STEP_SIZE_VALID)\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              epochs=30,\n",
    "                              callbacks=callback_list,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Performances of CNN\n",
    "1. Plot the learning curve for model validation to select the optimum hyperparameters\n",
    "2. Evaluating model performances using standard metrics e.g confusion matrix, accurancy score and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
